{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import itertools\n",
    "\n",
    "#import dataset\n",
    "data = pd.read_csv('pid-5M.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data vis 1: plot correlation grid (on the raw data)\n",
    "data.dataframeName = 'pid-5M.csv'\n",
    "\n",
    "def plotCorrelationMatrix(df, graphWidth):\n",
    "    filename = df.dataframeName\n",
    "    df = df.dropna('columns') # drop columns with NaN\n",
    "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
    "    if df.shape[1] < 2:\n",
    "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
    "        return\n",
    "    corr = df.corr()\n",
    "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
    "    corrMat = plt.matshow(corr, fignum = 1)\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
    "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "    plt.gca().xaxis.tick_bottom()\n",
    "    plt.colorbar(corrMat)\n",
    "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
    "    plt.show()\n",
    "\n",
    "plotCorrelationMatrix(data,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data vis 2: scatter plots for each feature against other features\n",
    "\n",
    "def plotScatterMatrix(df, plotSize, textSize):\n",
    "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
    "    # Remove rows and columns that would lead to df being singular\n",
    "    df = df.dropna('columns')\n",
    "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
    "    columnNames = list(df)\n",
    "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
    "        columnNames = columnNames[:10]\n",
    "    df = df[columnNames]\n",
    "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
    "    corrs = df.corr().values\n",
    "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
    "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
    "    plt.suptitle('Scatter and Density Plot')\n",
    "    plt.show()\n",
    "    \n",
    "plotScatterMatrix(data,20,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split up data \n",
    "\n",
    "D = np.array(data)\n",
    "X = D[:,1:7]\n",
    "y = D[:,0]\n",
    "\n",
    "#different modified data sets with training and testing data for each modification  --------------------------------\n",
    "\n",
    "# A) unmodified data set\n",
    "def datasetA():\n",
    "    return X, y\n",
    "\n",
    "# B) data set with all observations with zero values for inner or outer energy removed  -----------------------------\n",
    "def datasetB():\n",
    "    filterIx = (X[:,4] != 0) & (X[:,5] != 0)\n",
    "    return X[filterIx], y[filterIx]\n",
    "    \n",
    "# C) data set with all outer and inner energy features removed      -------------------------------------------\n",
    "def datasetC():\n",
    "    return X[:,0:4], y\n",
    "    \n",
    "# D) data set with zero values for outer and inner energy replaced with label averages for inner and outer energy\n",
    "def datasetD():\n",
    "    labels = [211., 321., -11., 2212.]\n",
    "    for label in labels:\n",
    "        label_features = D[y == label, 5:7]\n",
    "        label_averages = np.sum(label_features, axis=0)/label_features.shape[0]\n",
    "        D[np.where(D[:,5]==0), 5] = label_averages[0]\n",
    "        D[np.where(D[:,6]==0), 6] = label_averages[1]\n",
    "    return D[:,1:-1], y\n",
    "        \n",
    "# E) PCA     -------------------------------------------------------------------------------------------\n",
    "def datasetE():\n",
    "    from sklearn.decomposition import PCA\n",
    "    return PCA(svd_solver='full', n_components='mle').fit_transform(X), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# returns (accuracy score, # classes, running time)\n",
    "def run_model(model, dataset):\n",
    "    model_name = type(model).__name__\n",
    "    model_count = dataset[0]\n",
    "    from time import perf_counter\n",
    "    tick = perf_counter()\n",
    "    X_train, X_test, y_train, y_test = dataset[1]\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    tock = perf_counter()\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    result = accuracy_score(y_test, y_pred), np.unique(np.array(y_pred)), tock - tick\n",
    "    print(\"(\" + model_name + \" on data set \" + str(model_count) + \")\", result)\n",
    "    return result\n",
    "\n",
    "\n",
    "# returns (model, dataset) matrix of (accuracy score, # classes, running time) results\n",
    "def run_models(models, datasets):\n",
    "    print(\"Running models...\")\n",
    "    from multiprocessing import Pool\n",
    "    with Pool(processes = 4) as pool:\n",
    "        return np.array(\n",
    "            pool.starmap(run_model,[(model, dataset) for model in models for dataset in datasets])\n",
    "        ).reshape((len(models), len(datasets), 3))\n",
    "    \n",
    "    \n",
    "# Include models\n",
    "def build_models():\n",
    "    print(\"Building models...\")\n",
    "    lg_classifier = LogisticRegression(random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "    nb_classifier = GaussianNB()\n",
    "    nn_classifier = MLPClassifier(random_state = 0)\n",
    "    dt_classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "    rf_classifier = RandomForestClassifier(criterion = 'entropy', random_state = 0)\n",
    "    return [lg_classifier, nb_classifier, nn_classifier, dt_classifier, rf_classifier]\n",
    "    \n",
    "    \n",
    "def generate_dataset(i, dataset_generator):\n",
    "    X, y = dataset_generator()\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    dataset = X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "    while (len(np.unique(y_train)) != 4 or len(np.unique(y_test)) != 4):\n",
    "        dataset = X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "    return i, dataset\n",
    "\n",
    "\n",
    "# Include datasets (datasetX)\n",
    "def build_datasets():\n",
    "    print(\"Building datasets...\")\n",
    "    return [generate_dataset(i, dataset_generator) for i, dataset_generator in enumerate([datasetA, datasetB, datasetC, datasetD, datasetE])]\n",
    "\n",
    "# WIP\n",
    "def render_results(results):\n",
    "    print(\"Rendering results...\")\n",
    "    plt.matshow(results)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def run():\n",
    "    models = build_models()\n",
    "    datasets = build_datasets()\n",
    "    run_models(models, datasets)\n",
    "    \n",
    "    \n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
